{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a865620",
   "metadata": {},
   "source": [
    "## NTU Deep Learning Week Hackathon - 2024\n",
    "\n",
    "This notebook goes into depth about the backend services and technologies for TuneIn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8388f6",
   "metadata": {},
   "source": [
    "### Music Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4b357",
   "metadata": {},
   "source": [
    "### Video Generation\n",
    "\n",
    "ComfyUI workflow: https://www.youtube.com/@Ai_Davos/videos\n",
    "\n",
    "The idea behind this is to make perhaps more entertaining by adding on a visual element. This will help with spreading among social media algorithms.\n",
    "\n",
    "To achieve this, we want to have a dance choreography that is similar to that of the music that is generated, or at least similar to it. Here, we leverage Stanford Edge, https://github.com/Stanford-TML/EDGE, where we can generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e0976",
   "metadata": {},
   "source": [
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2f38b9-516e-4bfd-81f6-bbb671ea0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Directories\n",
    "output_folder = \"custom_music\"\n",
    "motion_folder = \"SMPL-to-FBX/motions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f4753c-1b84-4821-acec-96d6985f3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllist = [\n",
    "    \"https://www.youtube.com/watch?v=nsXwi67WgOo\",\n",
    "    \"https://www.youtube.com/watch?v=HCq1OcAEAm0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b067114-011d-40e5-85f3-84a216cec67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] nsXwi67WgOo: Downloading webpage\n",
      "[youtube] nsXwi67WgOo: Downloading player c48a9559\n",
      "[dashsegments] Total fragments: 1\n",
      "[download] Destination: custom_music/nsXwi67WgOo.webm\n",
      "\u001b[K[download] 100% of 2.20MiB in 00:00.48MiB/s ETA 00:002\n",
      "[ffmpeg] Destination: custom_music/nsXwi67WgOo.wav\n",
      "Deleting original file custom_music/nsXwi67WgOo.webm (pass -k to keep)\n",
      "[youtube] HCq1OcAEAm0: Downloading webpage\n",
      "[youtube] HCq1OcAEAm0: Downloading player 9bb09009\n",
      "[dashsegments] Total fragments: 1\n",
      "[download] Destination: custom_music/HCq1OcAEAm0.webm\n",
      "\u001b[K[download] 100% of 3.56MiB in 00:00.09MiB/s ETA 00:004\n",
      "[ffmpeg] Destination: custom_music/HCq1OcAEAm0.wav\n",
      "Deleting original file custom_music/HCq1OcAEAm0.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "for url in urllist:\n",
    "    !youtube-dl --extract-audio --audio-format wav --audio-quality 0 --output \"{output_folder}/%(id)s.%(ext)s\" \"{url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f7fea",
   "metadata": {},
   "source": [
    "We use the above two songs as a test to seee if we can effectively generate our dances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9b986-a395-4af8-a47b-9d573962b5b1",
   "metadata": {},
   "source": [
    "## 2. Generate Dances\n",
    "After the music is downloaded, run the model on the music to process the music and generate dances. Stick figure videos will be saved to `output_folder` and pickle files of the motions will be saved to `motion_folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea03e78-722c-4e76-b8c4-e0789785248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features for input music\n",
      "Slicing custom_music/HCq1OcAEAm0.wav\n",
      "Computing features for custom_music/HCq1OcAEAm0.wav\n",
      "  0%|                                                    | 0/84 [00:00<?, ?it/s]Importing jukebox and associated packages...\n",
      "Setting up the VQ-VAE...\n",
      "Loading vqvae in eval mode\n",
      "Setting up the top prior...\n",
      "Loading artist IDs from /opt/conda/lib/python3.10/site-packages/jukebox/data/ids/v2_artist_ids.txt\n",
      "Loading artist IDs from /opt/conda/lib/python3.10/site-packages/jukebox/data/ids/v2_genre_ids.txt\n",
      "Level:2, Cond downsample:None, Raw to tokens:128, Sample length:1048576\n",
      "Converting to fp16 params\n",
      "Loading prior in eval mode\n",
      "Loading the top prior weights into memory...\n",
      "\n",
      "  0%|                                                   | 0/872 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▊                                        | 16/872 [00:00<00:05, 156.27it/s]\u001b[A\n",
      "  9%|███▌                                     | 76/872 [00:00<00:03, 199.62it/s]\u001b[A\n",
      " 16%|██████▏                                 | 136/872 [00:00<00:02, 249.06it/s]\u001b[A\n",
      " 22%|████████▉                               | 196/872 [00:00<00:02, 301.49it/s]\u001b[A\n",
      " 29%|███████████▋                            | 256/872 [00:00<00:01, 353.72it/s]\u001b[A\n",
      " 36%|██████████████▍                         | 316/872 [00:00<00:01, 402.31it/s]\u001b[A\n",
      " 43%|█████████████████▏                      | 376/872 [00:00<00:01, 445.13it/s]\u001b[A\n",
      " 50%|████████████████████                    | 436/872 [00:00<00:00, 481.11it/s]\u001b[A\n",
      " 57%|██████████████████████▊                 | 496/872 [00:00<00:00, 510.18it/s]\u001b[A\n",
      " 64%|█████████████████████████▌              | 556/872 [00:01<00:00, 532.59it/s]\u001b[A\n",
      " 71%|████████████████████████████▎           | 616/872 [00:01<00:00, 548.73it/s]\u001b[A\n",
      " 78%|███████████████████████████████         | 676/872 [00:01<00:00, 560.74it/s]\u001b[A\n",
      " 84%|█████████████████████████████████▊      | 736/872 [00:01<00:00, 568.54it/s]\u001b[A\n",
      " 91%|████████████████████████████████████▌   | 796/872 [00:01<00:00, 574.98it/s]\u001b[A\n",
      "100%|████████████████████████████████████████| 872/872 [00:01<00:00, 545.62it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 669/669 [00:00<00:00, 18207.35it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 84/84 [01:05<00:00,  1.29it/s]\n",
      "Model has 49464471 parameters\n",
      "Generating dances\n",
      "sampling loop time step: 100%|██████████████████| 50/50 [00:01<00:00, 26.70it/s]\n",
      "/home/EDGE/vis.py:197: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  ax.scatter([], [], [], zorder=10, s=0, cmap=ListedColormap([\"r\", \"g\", \"b\"]))\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!python test.py --music_dir \"{output_folder}\"/ --save_motions --motion_save_dir \"{motion_folder}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b70117",
   "metadata": {},
   "source": [
    "This script automatically generates a 3D image. However there are two problems. One is that it is in 3D, and so it is difficult to use it to generate 'realistic' video. Additionally, another problem is that it doesn't follow a specific format. Thus, we'll convert this into OpenPose format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ca10900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('/home/poses.npy', 'rb') as f:\n",
    "    poses = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efacfa25",
   "metadata": {},
   "source": [
    "Here we'll try to map the 3-d video and cut it down into a front-facing 2d video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "54a3ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_parents = [\n",
    "\t-1,\n",
    "\t0,\n",
    "\t0,\n",
    "\t0,\n",
    "\t1,\n",
    "\t2,\n",
    "\t3,\n",
    "\t4,\n",
    "\t5,\n",
    "\t6,\n",
    "\t7,\n",
    "\t8,\n",
    "\t9,\n",
    "\t9,\n",
    "\t9,\n",
    "\t12,\n",
    "\t13,\n",
    "\t14,\n",
    "\t16,\n",
    "\t17,\n",
    "\t18,\n",
    "\t19,\n",
    "\t20,\n",
    "\t21,\n",
    "]\n",
    "\n",
    "smpl_offsets = np.array([\n",
    "\t[0.0, 0.0, 0.0],\n",
    "\t[0.05858135, -0.08228004, -0.01766408],\n",
    "\t[-0.06030973, -0.09051332, -0.01354254],\n",
    "\t[0.00443945, 0.12440352, -0.03838522],\n",
    "\t[0.04345142, -0.38646945, 0.008037],\n",
    "\t[-0.04325663, -0.38368791, -0.00484304],\n",
    "\t[0.00448844, 0.1379564, 0.02682033],\n",
    "\t[-0.01479032, -0.42687458, -0.037428],\n",
    "\t[0.01905555, -0.4200455, -0.03456167],\n",
    "\t[-0.00226458, 0.05603239, 0.00285505],\n",
    "\t[0.04105436, -0.06028581, 0.12204243],\n",
    "\t[-0.03483987, -0.06210566, 0.13032329],\n",
    "\t[-0.0133902, 0.21163553, -0.03346758],\n",
    "\t[0.07170245, 0.11399969, -0.01889817],\n",
    "\t[-0.08295366, 0.11247234, -0.02370739],\n",
    "\t[0.01011321, 0.08893734, 0.05040987],\n",
    "\t[0.12292141, 0.04520509, -0.019046],\n",
    "\t[-0.11322832, 0.04685326, -0.00847207],\n",
    "\t[0.2553319, -0.01564902, -0.02294649],\n",
    "\t[-0.26012748, -0.01436928, -0.03126873],\n",
    "\t[0.26570925, 0.01269811, -0.00737473],\n",
    "\t[-0.26910836, 0.00679372, -0.00602676],\n",
    "\t[0.08669055, -0.01063603, -0.01559429],\n",
    "\t[-0.0887537, -0.00865157, -0.01010708],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4c9afb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 1, 2, 4, 5, 10, 11, 16, 17, 18, 19, 20, 21, 15]\n"
     ]
    }
   ],
   "source": [
    "old_joints = np.array([\"root\",  \"lhip\",  \"rhip\",  \"belly\", \"lknee\", \"rknee\", \"spine\", \"lankle\",\"rankle\",\"chest\", \"ltoes\", \"rtoes\", \"neck\",  \"linshoulder\", \"rinshoulder\", \"head\", \"lshoulder\", \"rshoulder\", \"lelbow\", \"relbow\",  \"lwrist\", \"rwrist\", \"lhand\", \"rhand\"])\n",
    "new_joints = np.array(['neck', 'lhip',\t'rhip',\t'lknee', 'rknee',\t'ltoes',\t'rtoes',\t'lshoulder',\t'rshoulder',\t'lelbow',\t'relbow',\t'lwrist', 'rwrist', 'head'])\n",
    "\n",
    "# Step 1: Sort the first array and remember the original indices\n",
    "sorted_indices = np.argsort(old_joints)\n",
    "sorted_arr1 = old_joints[sorted_indices]\n",
    "\n",
    "# Step 2: Find positions of the second array's elements in the sorted first array\n",
    "positions = np.searchsorted(sorted_arr1, new_joints)\n",
    "\n",
    "# Step 3: Map these positions back to the original indices\n",
    "new_idx = sorted_indices[positions]\n",
    "new_idx = new_idx.tolist()\n",
    "print(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b4ac78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_joints = [\n",
    "\t'neck', # 0\n",
    "\t'lhip', # 1\n",
    "\t'rhip', # 2\n",
    "\t'lknee', # 3\n",
    "\t'rknee', # 4\n",
    "\t'ltoes', # 5\n",
    "\t'rtoes', # 6\n",
    "'lshoulder', # 7\n",
    "\t'rshoulder', # 8\n",
    "\t'lelbow', # 9\n",
    "\t'relbow', # 10\n",
    "\t'lwrist', # 11\n",
    "\t'rwrist', # 12\n",
    "\t'head',\n",
    "]\n",
    "\n",
    "smpl_parents = [\n",
    "\t-1,\n",
    "\t0,\n",
    "\t0,\n",
    "\t1,\n",
    "\t2,\n",
    "\t3,\n",
    "\t4,\n",
    "\t0,\n",
    "\t0,\n",
    "\t7,\n",
    "\t8,\n",
    "\t9,\n",
    "\t10,\n",
    "\t0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3d3e5345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/Gen/opencv/0.png',\n",
       " '/home/Gen/opencv/1.png',\n",
       " '/home/Gen/opencv/2.png',\n",
       " '/home/Gen/opencv/3.png',\n",
       " '/home/Gen/opencv/4.png',\n",
       " '/home/Gen/opencv/5.png',\n",
       " '/home/Gen/opencv/6.png',\n",
       " '/home/Gen/opencv/7.png',\n",
       " '/home/Gen/opencv/8.png',\n",
       " '/home/Gen/opencv/9.png',\n",
       " '/home/Gen/opencv/10.png',\n",
       " '/home/Gen/opencv/11.png',\n",
       " '/home/Gen/opencv/12.png',\n",
       " '/home/Gen/opencv/13.png',\n",
       " '/home/Gen/opencv/14.png',\n",
       " '/home/Gen/opencv/15.png',\n",
       " '/home/Gen/opencv/16.png',\n",
       " '/home/Gen/opencv/17.png',\n",
       " '/home/Gen/opencv/18.png',\n",
       " '/home/Gen/opencv/19.png',\n",
       " '/home/Gen/opencv/20.png',\n",
       " '/home/Gen/opencv/21.png',\n",
       " '/home/Gen/opencv/22.png',\n",
       " '/home/Gen/opencv/23.png',\n",
       " '/home/Gen/opencv/24.png',\n",
       " '/home/Gen/opencv/25.png',\n",
       " '/home/Gen/opencv/26.png',\n",
       " '/home/Gen/opencv/27.png',\n",
       " '/home/Gen/opencv/28.png',\n",
       " '/home/Gen/opencv/29.png',\n",
       " '/home/Gen/opencv/30.png',\n",
       " '/home/Gen/opencv/31.png',\n",
       " '/home/Gen/opencv/32.png',\n",
       " '/home/Gen/opencv/33.png',\n",
       " '/home/Gen/opencv/34.png',\n",
       " '/home/Gen/opencv/35.png',\n",
       " '/home/Gen/opencv/36.png',\n",
       " '/home/Gen/opencv/37.png',\n",
       " '/home/Gen/opencv/38.png',\n",
       " '/home/Gen/opencv/39.png',\n",
       " '/home/Gen/opencv/40.png',\n",
       " '/home/Gen/opencv/41.png',\n",
       " '/home/Gen/opencv/42.png',\n",
       " '/home/Gen/opencv/43.png',\n",
       " '/home/Gen/opencv/44.png',\n",
       " '/home/Gen/opencv/45.png',\n",
       " '/home/Gen/opencv/46.png',\n",
       " '/home/Gen/opencv/47.png',\n",
       " '/home/Gen/opencv/48.png',\n",
       " '/home/Gen/opencv/49.png',\n",
       " '/home/Gen/opencv/50.png',\n",
       " '/home/Gen/opencv/51.png',\n",
       " '/home/Gen/opencv/52.png',\n",
       " '/home/Gen/opencv/53.png',\n",
       " '/home/Gen/opencv/54.png',\n",
       " '/home/Gen/opencv/55.png',\n",
       " '/home/Gen/opencv/56.png',\n",
       " '/home/Gen/opencv/57.png',\n",
       " '/home/Gen/opencv/58.png',\n",
       " '/home/Gen/opencv/59.png',\n",
       " '/home/Gen/opencv/60.png',\n",
       " '/home/Gen/opencv/61.png',\n",
       " '/home/Gen/opencv/62.png',\n",
       " '/home/Gen/opencv/63.png',\n",
       " '/home/Gen/opencv/64.png',\n",
       " '/home/Gen/opencv/65.png',\n",
       " '/home/Gen/opencv/66.png',\n",
       " '/home/Gen/opencv/67.png',\n",
       " '/home/Gen/opencv/68.png',\n",
       " '/home/Gen/opencv/69.png',\n",
       " '/home/Gen/opencv/70.png',\n",
       " '/home/Gen/opencv/71.png',\n",
       " '/home/Gen/opencv/72.png',\n",
       " '/home/Gen/opencv/73.png',\n",
       " '/home/Gen/opencv/74.png',\n",
       " '/home/Gen/opencv/75.png',\n",
       " '/home/Gen/opencv/76.png',\n",
       " '/home/Gen/opencv/77.png',\n",
       " '/home/Gen/opencv/78.png',\n",
       " '/home/Gen/opencv/79.png',\n",
       " '/home/Gen/opencv/80.png',\n",
       " '/home/Gen/opencv/81.png',\n",
       " '/home/Gen/opencv/82.png',\n",
       " '/home/Gen/opencv/83.png',\n",
       " '/home/Gen/opencv/84.png',\n",
       " '/home/Gen/opencv/85.png',\n",
       " '/home/Gen/opencv/86.png',\n",
       " '/home/Gen/opencv/87.png',\n",
       " '/home/Gen/opencv/88.png',\n",
       " '/home/Gen/opencv/89.png',\n",
       " '/home/Gen/opencv/90.png',\n",
       " '/home/Gen/opencv/91.png',\n",
       " '/home/Gen/opencv/92.png',\n",
       " '/home/Gen/opencv/93.png',\n",
       " '/home/Gen/opencv/94.png',\n",
       " '/home/Gen/opencv/95.png',\n",
       " '/home/Gen/opencv/96.png',\n",
       " '/home/Gen/opencv/97.png',\n",
       " '/home/Gen/opencv/98.png',\n",
       " '/home/Gen/opencv/99.png',\n",
       " '/home/Gen/opencv/100.png',\n",
       " '/home/Gen/opencv/101.png',\n",
       " '/home/Gen/opencv/102.png',\n",
       " '/home/Gen/opencv/103.png',\n",
       " '/home/Gen/opencv/104.png',\n",
       " '/home/Gen/opencv/105.png',\n",
       " '/home/Gen/opencv/106.png',\n",
       " '/home/Gen/opencv/107.png',\n",
       " '/home/Gen/opencv/108.png',\n",
       " '/home/Gen/opencv/109.png',\n",
       " '/home/Gen/opencv/110.png',\n",
       " '/home/Gen/opencv/111.png',\n",
       " '/home/Gen/opencv/112.png',\n",
       " '/home/Gen/opencv/113.png',\n",
       " '/home/Gen/opencv/114.png',\n",
       " '/home/Gen/opencv/115.png',\n",
       " '/home/Gen/opencv/116.png',\n",
       " '/home/Gen/opencv/117.png',\n",
       " '/home/Gen/opencv/118.png',\n",
       " '/home/Gen/opencv/119.png',\n",
       " '/home/Gen/opencv/120.png',\n",
       " '/home/Gen/opencv/121.png',\n",
       " '/home/Gen/opencv/122.png',\n",
       " '/home/Gen/opencv/123.png',\n",
       " '/home/Gen/opencv/124.png',\n",
       " '/home/Gen/opencv/125.png',\n",
       " '/home/Gen/opencv/126.png',\n",
       " '/home/Gen/opencv/127.png',\n",
       " '/home/Gen/opencv/128.png',\n",
       " '/home/Gen/opencv/129.png',\n",
       " '/home/Gen/opencv/130.png',\n",
       " '/home/Gen/opencv/131.png',\n",
       " '/home/Gen/opencv/132.png',\n",
       " '/home/Gen/opencv/133.png',\n",
       " '/home/Gen/opencv/134.png',\n",
       " '/home/Gen/opencv/135.png',\n",
       " '/home/Gen/opencv/136.png',\n",
       " '/home/Gen/opencv/137.png',\n",
       " '/home/Gen/opencv/138.png',\n",
       " '/home/Gen/opencv/139.png',\n",
       " '/home/Gen/opencv/140.png',\n",
       " '/home/Gen/opencv/141.png',\n",
       " '/home/Gen/opencv/142.png',\n",
       " '/home/Gen/opencv/143.png',\n",
       " '/home/Gen/opencv/144.png',\n",
       " '/home/Gen/opencv/145.png',\n",
       " '/home/Gen/opencv/146.png',\n",
       " '/home/Gen/opencv/147.png',\n",
       " '/home/Gen/opencv/148.png',\n",
       " '/home/Gen/opencv/149.png',\n",
       " '/home/Gen/opencv/150.png',\n",
       " '/home/Gen/opencv/151.png',\n",
       " '/home/Gen/opencv/152.png',\n",
       " '/home/Gen/opencv/153.png',\n",
       " '/home/Gen/opencv/154.png',\n",
       " '/home/Gen/opencv/155.png',\n",
       " '/home/Gen/opencv/156.png',\n",
       " '/home/Gen/opencv/157.png',\n",
       " '/home/Gen/opencv/158.png',\n",
       " '/home/Gen/opencv/159.png',\n",
       " '/home/Gen/opencv/160.png',\n",
       " '/home/Gen/opencv/161.png',\n",
       " '/home/Gen/opencv/162.png',\n",
       " '/home/Gen/opencv/163.png',\n",
       " '/home/Gen/opencv/164.png',\n",
       " '/home/Gen/opencv/165.png',\n",
       " '/home/Gen/opencv/166.png',\n",
       " '/home/Gen/opencv/167.png',\n",
       " '/home/Gen/opencv/168.png',\n",
       " '/home/Gen/opencv/169.png',\n",
       " '/home/Gen/opencv/170.png',\n",
       " '/home/Gen/opencv/171.png',\n",
       " '/home/Gen/opencv/172.png',\n",
       " '/home/Gen/opencv/173.png',\n",
       " '/home/Gen/opencv/174.png',\n",
       " '/home/Gen/opencv/175.png',\n",
       " '/home/Gen/opencv/176.png',\n",
       " '/home/Gen/opencv/177.png',\n",
       " '/home/Gen/opencv/178.png',\n",
       " '/home/Gen/opencv/179.png',\n",
       " '/home/Gen/opencv/180.png',\n",
       " '/home/Gen/opencv/181.png',\n",
       " '/home/Gen/opencv/182.png',\n",
       " '/home/Gen/opencv/183.png',\n",
       " '/home/Gen/opencv/184.png',\n",
       " '/home/Gen/opencv/185.png',\n",
       " '/home/Gen/opencv/186.png',\n",
       " '/home/Gen/opencv/187.png',\n",
       " '/home/Gen/opencv/188.png',\n",
       " '/home/Gen/opencv/189.png',\n",
       " '/home/Gen/opencv/190.png',\n",
       " '/home/Gen/opencv/191.png',\n",
       " '/home/Gen/opencv/192.png',\n",
       " '/home/Gen/opencv/193.png',\n",
       " '/home/Gen/opencv/194.png',\n",
       " '/home/Gen/opencv/195.png',\n",
       " '/home/Gen/opencv/196.png',\n",
       " '/home/Gen/opencv/197.png',\n",
       " '/home/Gen/opencv/198.png',\n",
       " '/home/Gen/opencv/199.png']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Assuming poses, smpl_offsets, and other required variables are initialized elsewhere\n",
    "\n",
    "num_joints = 24  # Adjust based on your actual model\n",
    "images_paths = []\n",
    "\n",
    "new_poses = poses[:, new_idx, :]\n",
    "new_offset = smpl_offsets[new_idx]\n",
    "\n",
    "marker = {\n",
    "\t'neck': 'FF5500',\n",
    "\t'lhip': '0055FF',\n",
    "\t'rhip': '00FFAA',\n",
    "\t'lknee': '0000FF',\n",
    "\t'rknee': '00FFFF',\n",
    "\t'ltoes': '5500FF',\n",
    "\t'rtoes': '00AAFF',\n",
    "\t'lshoulder': '55FF00',\n",
    "\t'rshoulder': 'FFAA00',\n",
    "\t'lelbow': '00FF00',\n",
    "\t'relbow': 'FFFF00',\n",
    "\t'lwrist': '00FF55',\n",
    "\t'rwrist': 'AAFF00',\n",
    "\t'head': 'FF0000',\n",
    "\t'leye': 'FF00FF',\n",
    "\t'lear': 'FF0055',\n",
    "\t'reye': 'AA00FF',\n",
    "\t'rear': 'FF00AA'\n",
    "}\n",
    "\n",
    "bone = {\n",
    "\t'lhip': '009999',\n",
    "\t'rhip': '009900',\n",
    "\t'lknee': '006699',\n",
    "\t'rknee': '009933',\n",
    "\t'ltoes': '003399',\n",
    "\t'rtoes': '009966',\n",
    "\t'lshoulder': '993300',\n",
    "\t'rshoulder': '990000',\n",
    "\t'lelbow': '669900',\n",
    "\t'relbow': '996600',\n",
    "\t'lwrist': '339900',\n",
    "\t'rwrist': '999900',\n",
    "\t'head': '000099',\n",
    "\t'leye': '990099',\n",
    "\t'lear': '990066',\n",
    "\t'reye': '330099',\n",
    "\t'rear': '660099'\n",
    "}\n",
    "\n",
    "\n",
    "# Function to draw the skeleton\n",
    "def dfs(u, par, num, img):\n",
    "\tif par != -1:\n",
    "\t\t# Convert color hex to BGR\n",
    "\n",
    "\t\tline_color = tuple(int(bone[smpl_joints[u]][i:i+2], 16) for i in (4, 2, 0))\n",
    "\t\t# Draw line for bone\n",
    "\t\tcv2.line(img,\n",
    "\t\t\t\t (int((new_offset[par, 0] + new_poses[num, par, 0] + 1.5) * 512 / 3), 100 + 768 - int((new_offset[par, 2] + new_poses[num, par, 2]) * 768 / 3)),\n",
    "\t\t\t\t (int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5) * 512 / 3), 100 + 768 - int((new_offset[u, 2] + new_poses[num, u, 2]) * 768 / 3)),\n",
    "\t\t\t\t line_color, 2)\n",
    "\t\t\n",
    "\tjoint_color = tuple(int(marker[smpl_joints[u]][i:i+2], 16) for i in (4, 2, 0))\n",
    "\t# Draw joint\n",
    "\tcv2.circle(img,\n",
    "\t\t\t\t(int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5) * 512 / 3), 100 + 768 - int((new_offset[u, 2] + new_poses[num, u, 2]) * 768 / 3)),\n",
    "\t\t\t\t3, joint_color, -1)\n",
    "\n",
    "\tif smpl_joints[u] == 'head':\n",
    "\t\tcv2.line(img,\n",
    "\t\t\t\t(int((new_offset[par, 0] + new_poses[num, par, 0] + 1.5) * 512 / 3), 100 + 768 - int((new_offset[par, 2] + new_poses[num, par, 2]) * 768 / 3)),\n",
    "\t\t\t\t(int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5) * 512 / 3), 100 + 768 - int((new_offset[u, 2] + new_poses[num, u, 2]) * 768 / 3)),\n",
    "\t\t\t\tline_color, 2)\n",
    "\t\t\n",
    "\t\tface_points = {\n",
    "\t\t\t'reye': int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5 - 0.07) * 512 / 3), \n",
    "\t\t\t'rear': int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5 - 0.12) * 512 / 3), \n",
    "\t\t\t'leye': int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5 + 0.07) * 512 / 3), \n",
    "\t\t\t'lear': int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5 + 0.12) * 512 / 3),\n",
    "\t\t}\n",
    "\t\tnames = ['leye', 'lear', 'reye', 'rear']\n",
    "\n",
    "\t\tpar_y_point = 100 + 768 - int((new_offset[u, 2] + new_poses[num, u, 2]) * 768 / 3)\n",
    "\t\ty_point = 100 + 768 - int((new_offset[u, 2] + new_poses[num, u, 2] + 0.05) * 768 / 3)\n",
    "\n",
    "\t\tfor name in names:\n",
    "\n",
    "\n",
    "\t\t\tjoint_color = tuple(int(marker[name][i:i+2], 16) for i in (4, 2, 0))\n",
    "\t\t\tline_color = tuple(int(bone[name][i:i+2], 16) for i in (4, 2, 0))\n",
    "\t\t\tfp = face_points[name]\n",
    "\t\t\t# Draw joint\n",
    "\n",
    "\t\t\tif name in ['leye', 'reye']:\n",
    "\t\t\t\tpx, py = (int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5) * 512 / 3), 100 + 768 - int((new_offset[u, 2] + new_poses[num, u, 2]) * 768 / 3))\n",
    "\t\t\telif name == 'rear':\n",
    "\t\t\t\tpx = face_points['reye']\n",
    "\t\t\t\tpy = y_point\n",
    "\t\t\telse:\n",
    "\t\t\t\tpx = face_points['leye']\n",
    "\t\t\t\tpy = y_point\n",
    "\n",
    "\t\t\tcv2.line(img,\n",
    "\t\t\t\t(px, py),\n",
    "\t\t\t\t(fp, y_point),\n",
    "\t\t\t\tline_color, 2)\n",
    "\n",
    "\t\t\tcv2.circle(img,\n",
    "\t\t\t\t\t(fp, y_point),\n",
    "\t\t\t\t\t3, joint_color, -1)\n",
    "\n",
    "\t\tfor d in [-0.07, 0.07]:\n",
    "\t\t\t# Assuming img, new_offset, new_poses, and y_point are defined as per your context\n",
    "\t\t\t# Example parameters for the ellipse\n",
    "\t\t\tcenter_x = int((new_offset[u, 0] + new_poses[num, u, 0] + 1.5 - d) * 512 / 3)\n",
    "\t\t\tcenter_y = y_point - 5\n",
    "\t\t\taxes_length = (4, 2)  # Major and minor axes lengths\n",
    "\n",
    "\t\t\t# Draw the ellipse (eye)\n",
    "\t\t\tcv2.ellipse(img, (center_x, center_y), axes_length,\n",
    "\t\t\t\t\t\t0,  # Rotation angle\n",
    "\t\t\t\t\t\t0,  # Starting angle\n",
    "\t\t\t\t\t\t360,  # Ending angle\n",
    "\t\t\t\t\t\t(255, 255, 255),  # Color (white)\n",
    "\t\t\t\t\t\t1)  # Thickness (-1 for filled)\n",
    "\n",
    "\t\t\t# Calculate points around the ellipse for placing circles\n",
    "\t\t\tangles = np.linspace(0, 2 * np.pi, 7)[:-1]  # Exclude the last point as it's the same as the first\n",
    "\t\t\tcircle_radius = 2  # Radius of the circles to be drawn\n",
    "\n",
    "\t\t\tfor angle in angles:\n",
    "\t\t\t\t# For points on the ellipse: x = a * cos(t), y = b * sin(t), where a and b are the semi-axes lengths\n",
    "\t\t\t\t# Adjusting for the center of the ellipse\n",
    "\t\t\t\tx = center_x + axes_length[0] * np.cos(angle)\n",
    "\t\t\t\ty = center_y + axes_length[1] * np.sin(angle)\n",
    "\n",
    "\t\t\t\t# Draw circles at calculated positions\n",
    "\t\t\t\tcv2.circle(img, (int(x), int(y)), circle_radius, (255, 255, 255), -1)  # Color (red) for visibility\n",
    "\n",
    "\tidx = np.where(pars == u)[0]\n",
    "\tfor c in idx:\n",
    "\t\tif not vis[c]:\n",
    "\t\t\tdfs(c, u, num, img)\n",
    "\n",
    "# Creating a folder to save the images\n",
    "image_dir = \"/home/Gen/opencv\"  # Update to your path\n",
    "if not os.path.exists(image_dir):\n",
    "\tos.makedirs(image_dir)\n",
    "\n",
    "for num in range(200):  # Generate images for each num\n",
    "\t# Create an empty black image\n",
    "\timg = np.zeros((768, 512, 3), dtype=np.uint8)\n",
    "\n",
    "\t# Initialize visited array for DFS\n",
    "\tvis = np.zeros(num_joints, dtype=bool)\n",
    "\n",
    "\t# Call DFS to draw the skeleton on the image\n",
    "\tdfs(0, -1, num, img)\n",
    "\n",
    "\t# Save the image\n",
    "\timage_path = f\"{image_dir}/{num}.png\"\n",
    "\tcv2.imwrite(image_path, img)\n",
    "\timages_paths.append(image_path)\n",
    "\n",
    "images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f197493f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/smpl_animation.gif'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Create a GIF from the saved images\n",
    "gif_path = \"/home/smpl_animation.gif\"\n",
    "frames = [Image.open(image_path) for image_path in images_paths]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(gif_path, format=\"GIF\", append_images=frames,\n",
    "               save_all=True, duration=20, loop=0)\n",
    "\n",
    "gif_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d36873",
   "metadata": {},
   "source": [
    "<img src=\"smpl_animation.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bf624",
   "metadata": {},
   "source": [
    "Once we have the OpenPose details, we moved over to ComfyUI to make the flow, as well as generate an API.\n",
    "Specifically, we use the above OpenPose images and combine it with AnimateAnyone, as well as ControlNets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6544c0",
   "metadata": {},
   "source": [
    "<img src=\"comfy.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a708d8",
   "metadata": {},
   "source": [
    "<img src=\"comfy2.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173187a",
   "metadata": {},
   "source": [
    "This is then transformed into a REST API Endpoint, for us to ping with the relevant information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
